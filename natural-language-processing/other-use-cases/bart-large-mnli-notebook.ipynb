{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f37d919-8e25-4149-9f94-6aeebce8d2cd",
   "metadata": {},
   "source": [
    "# SQuAD and MNLI on IPUs using BART-LARGE - Inference\n",
    "This notebook provides an implementation of two natural language understanding (NLU) tasks using small, efficient models: [Facebook BART-LARGE](https://huggingface.co/facebook/bart-large-mnli) for sequence classification and question answering. The notebook demonstrates how these models can achieve good performance on standard benchmarks while being relatively lightweight and easy to use.\n",
    "\n",
    "The two NLU tasks covered in this notebook are:\n",
    "- Multi-Genre Natural Language Inference (MNLI) - a sentence-pair classification task\n",
    "Hardware requirements: The models show each BART-Large model running on two IPUs. \n",
    "\n",
    "[![Run on Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/internetoftim/Gradient-HuggingFace?machine=Free-IPU-POD4&container=graphcore/pytorch-jupyter%3A3.2.1-ubuntu-20.04-20230531&file=natural-language-processing%2Fother-use-cases%2Fbart-large-mnli-notebook.ipynb)  [![Join our Slack Community](https://img.shields.io/badge/Slack-Join%20Graphcore's%20Community-blue?style=flat-square&logo=slack)](https://www.graphcore.ai/join-community)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe52060",
   "metadata": {},
   "source": [
    "##### Optimum Graphcore\n",
    "The notebook also demonstrates [Optimum Graphcore](https://github.com/huggingface/optimum-graphcore). Optimum Graphcore is the interface between the Hugging Face Transformers library and [Graphcore IPUs](https://www.graphcore.ai/products/ipu). This notebook demonstrates a more explicit way of using Huggingface models with the IPU. This method is particularly useful when the task in question is not supported by the Huggingface pipelines API.\n",
    "\n",
    "The easiest way to run a Huggingface inference model would be to instantiate the pipeline as follows:\n",
    "\n",
    "```\n",
    "oracle = pipeline(model=\"Palak/microsoft_deberta-base_squad\")\n",
    "oracle(question=\"Where do I live?\", context=\"My name is Wolfgang and I live in Berlin\")\n",
    "```\n",
    "\n",
    "However in some cases such as MNLI, there is no off-the-shelf pipeline ready to use. In this case, you could simply:\n",
    "- Instantiate the model with the correct execution mode\n",
    "- Use the optimum-specific call `to_pipelined` to return the model with changes and annotations for running on the IPU\n",
    "- Set the model to run in `eval` mode and use the `parallelize` method on the new model to parallelize it across IPUs\n",
    "- Prepare it for inference using `poptorch.inferenceModel()`\n",
    "\n",
    "```\n",
    "model = DebertaForQuestionAnswering.from_pretrained(\"Palak/microsoft_deberta-base_squad\")\n",
    "\n",
    "ipu_config = IPUConfig(ipus_per_replica=2, matmul_proportion=0.2, executable_cache_dir=\"./exe_cache\")\n",
    "pipelined_model = to_pipelined(model, ipu_config).eval().parallelize()\n",
    "pipelined_model = poptorch.inferenceModel(pipelined_model, options=ipu_config.to_options(for_inference=True))\n",
    "```\n",
    "\n",
    "This method is demoed in this notebook, as Huggingface do not natively support the MNLI inference task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227fd68-3108-4ac2-9ef2-b1fbbe069d74",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install the optimum library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d132efc-0d4a-4647-af51-f4bde32eeeb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T16:38:03.509778Z",
     "iopub.status.busy": "2023-07-11T16:38:03.509482Z",
     "iopub.status.idle": "2023-07-11T16:38:45.116250Z",
     "shell.execute_reply": "2023-07-11T16:38:45.115465Z",
     "shell.execute_reply.started": "2023-07-11T16:38:03.509755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum-graphcore==0.6.1\n",
      "  Downloading optimum_graphcore-0.6.1-py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.25.1 (from optimum-graphcore==0.6.1)\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting optimum==1.6.1 (from optimum-graphcore==0.6.1)\n",
      "  Downloading optimum-1.6.1-py3-none-any.whl (222 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.6/222.6 kB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diffusers[torch]==0.12.1 (from optimum-graphcore==0.6.1)\n",
      "  Downloading diffusers-0.12.1-py3-none-any.whl (604 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m604.0/604.0 kB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from optimum-graphcore==0.6.1)\n",
      "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m135.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers (from optimum-graphcore==0.6.1)\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting typeguard (from optimum-graphcore==0.6.1)\n",
      "  Downloading typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
      "Collecting sentencepiece (from optimum-graphcore==0.6.1)\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from optimum-graphcore==0.6.1)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow (from optimum-graphcore==0.6.1)\n",
      "  Downloading Pillow-10.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (6.6.0)\n",
      "Collecting filelock (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub>=0.10.0 (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Downloading regex-2023.6.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (2.31.0)\n",
      "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.8/dist-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (1.13.1+cpu)\n",
      "Collecting accelerate>=0.11.0 (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting coloredlogs (from optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy (from optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers[sentencepiece]>=4.20.1 (from optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum==1.6.1->optimum-graphcore==0.6.1) (23.1)\n",
      "Collecting numpy (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1->optimum-graphcore==0.6.1) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1->optimum-graphcore==0.6.1) (4.65.0)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0 (from datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1 (from datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.8/dist-packages (from typeguard->optimum-graphcore==0.6.1) (4.6.2)\n",
      "Collecting psutil (from accelerate>=0.11.0->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore==0.6.1) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore==0.6.1) (3.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.9/266.9 kB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (3.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (2019.11.28)\n",
      "INFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers[sentencepiece]>=4.20.1 (from optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25h  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25h  Downloading transformers-4.27.0-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<=3.20.2 (from transformers==4.25.1->optimum-graphcore==0.6.1)\n",
      "  Downloading protobuf-3.20.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-graphcore==0.6.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-graphcore==0.6.1) (2023.3)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets->optimum-graphcore==0.6.1)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath>=0.19 (from sympy->optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum-graphcore==0.6.1) (1.14.0)\n",
      "Installing collected packages: tokenizers, sentencepiece, mpmath, xxhash, tzdata, sympy, regex, psutil, protobuf, pillow, numpy, multidict, humanfriendly, fsspec, frozenlist, filelock, dill, async-timeout, yarl, typeguard, scipy, pyarrow, pandas, multiprocess, huggingface-hub, coloredlogs, aiosignal, accelerate, transformers, diffusers, aiohttp, optimum, datasets, optimum-graphcore\n",
      "Successfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 coloredlogs-15.0.1 datasets-2.13.1 diffusers-0.12.1 dill-0.3.6 filelock-3.12.2 frozenlist-1.3.3 fsspec-2023.6.0 huggingface-hub-0.16.4 humanfriendly-10.0 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.14 numpy-1.23.5 optimum-1.6.1 optimum-graphcore-0.6.1 pandas-2.0.3 pillow-10.0.0 protobuf-3.20.2 psutil-5.9.5 pyarrow-12.0.1 regex-2023.6.3 scipy-1.10.1 sentencepiece-0.1.99 sympy-1.12 tokenizers-0.13.3 transformers-4.25.1 typeguard-4.0.0 tzdata-2023.3 xxhash-3.2.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"optimum-graphcore==0.6.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed09d3",
   "metadata": {},
   "source": [
    "We read some configuration from the environment to support environments like Paperspace Gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ca39a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T16:38:45.117621Z",
     "iopub.status.busy": "2023-07-11T16:38:45.117438Z",
     "iopub.status.idle": "2023-07-11T16:38:45.121664Z",
     "shell.execute_reply": "2023-07-11T16:38:45.120583Z",
     "shell.execute_reply.started": "2023-07-11T16:38:45.117603Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"./exe_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014263b-9a6e-4c94-8a0f-8b692fa67bc6",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271eb456-4392-4471-8540-510ed2048a27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T16:38:45.123514Z",
     "iopub.status.busy": "2023-07-11T16:38:45.123258Z",
     "iopub.status.idle": "2023-07-11T16:38:47.188267Z",
     "shell.execute_reply": "2023-07-11T16:38:47.187550Z",
     "shell.execute_reply.started": "2023-07-11T16:38:45.123487Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "import poptorch\n",
    "from optimum.graphcore import IPUConfig\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizerFast, BartForSequenceClassification\n",
    "from transformers import DebertaForSequenceClassification, DebertaTokenizerFast\n",
    "from transformers import DebertaForQuestionAnswering, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d94da7-894b-4e98-a52f-eaab56b55b80",
   "metadata": {},
   "source": [
    "## Multi-Genre Natural Language Inference (MNLI)\n",
    "\n",
    "MNLI is a sentence-pair classification task, where the goal is to predict whether a given hypothesis is true (entailment) or false (contradiction) given a premise. The task has been proposed as a benchmark for evaluating natural language understanding models. \n",
    "\n",
    "In this notebook, we use the Facebook/BART-large model to classify pairs of sentences on the MNLI task. We first load the model and the tokenizer, then prepare an example input. Finally, we execute the model on an IPU device using PopTorch and obtain the predicted probabilities for the entailment classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e87ec8",
   "metadata": {},
   "source": [
    "First, load the model and tokeniser from the Huggingface Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d336657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T16:47:39.352570Z",
     "iopub.status.busy": "2023-07-11T16:47:39.352308Z",
     "iopub.status.idle": "2023-07-11T16:47:45.740356Z",
     "shell.execute_reply": "2023-07-11T16:47:45.739631Z",
     "shell.execute_reply.started": "2023-07-11T16:47:39.352550Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base-mnli\")\n",
    "# model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base-mnli\")\n",
    "# model.half()\n",
    "\n",
    "\n",
    "from transformers import BartForSequenceClassification\n",
    "\n",
    "model_checkpoint = \"facebook/bart-large-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "model = BartForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd484d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0b186f7",
   "metadata": {},
   "source": [
    "Create some example inputs, and encoder those using the tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27fdc19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T16:39:42.471194Z",
     "iopub.status.busy": "2023-07-11T16:39:42.471021Z",
     "iopub.status.idle": "2023-07-11T16:39:42.480887Z",
     "shell.execute_reply": "2023-07-11T16:39:42.480048Z",
     "shell.execute_reply.started": "2023-07-11T16:39:42.471176Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2371: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "premise = \"A man inspects the uniform of a figure in some East Asian country.\"\n",
    "hypothesis = \"The man is in an East Asian country.\"\n",
    "\n",
    "inputs = tokenizer.encode(\n",
    "    premise, hypothesis, return_tensors=\"pt\", truncation_strategy=\"only_first\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5dbbd4",
   "metadata": {},
   "source": [
    "Configure the instantiated model to run on IPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12b347ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T16:48:05.488226Z",
     "iopub.status.busy": "2023-07-11T16:48:05.487836Z",
     "iopub.status.idle": "2023-07-11T16:48:13.653798Z",
     "shell.execute_reply": "2023-07-11T16:48:13.653090Z",
     "shell.execute_reply.started": "2023-07-11T16:48:05.488205Z"
    }
   },
   "outputs": [],
   "source": [
    "# ipu_config = IPUConfig(ipus_per_replica=2, matmul_proportion=0.6, executable_cache_dir=executable_cache_dir)\n",
    "ipu_config = IPUConfig(layers_per_ipu=[0,12,6,6], ipus_per_replica=4, matmul_proportion=0.6, executable_cache_dir=executable_cache_dir)\n",
    "# ipu_config = IPUConfig(layers_per_ipu=[8,16], ipus_per_replica=2,replication_factor=2, matmul_proportion=0.6, executable_cache_dir=executable_cache_dir)\n",
    "\n",
    "pipelined_model = to_pipelined(model, ipu_config).eval().parallelize()\n",
    "pipelined_model = poptorch.inferenceModel(pipelined_model, options=ipu_config.to_options(for_inference=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514245c3",
   "metadata": {},
   "source": [
    "Run the MNLI model and print the probability of entailment. We calculate this by throwing away neutral (index 1) and running softmax over the remaining logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77176c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T16:48:13.654912Z",
     "iopub.status.busy": "2023-07-11T16:48:13.654729Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation:  55%|█████▌    | 55/100 [01:03<00:33]"
     ]
    }
   ],
   "source": [
    "logits = pipelined_model(inputs)[0]\n",
    "entail_contradiction_logits = logits[:, [0, 2]]\n",
    "prob_label_is_true = entail_contradiction_logits.softmax(dim=1)[:, 1]\n",
    "print(prob_label_is_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
